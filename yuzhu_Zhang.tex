%=======================02-713 LaTeX template, following the 15-210 template==================
%
% You don't need to use LaTeX or this template, but you must turn your homework in as
% a typeset PDF somehow.
%
% How to use:
%    1. Update your information in section "A" below
%    2. Write your answers in section "B" below. Precede answers for all 
%       parts of a question with the command "\question{n}{desc}" where n is
%       the question number and "desc" is a short, one-line description of 
%       the problem. There is no need to restate the problem.
%    3. If a question has multiple parts, precede the answer to part x with the
%       command "\part{x}".
%    4. If a problem asks you to design an algorithm, use the commands
%       \algorithm, \correctness, \runtime to precede your discussion of the 
%       description of the algorithm, its correctness, and its running time, respectively.
%    5. You can include graphics by using the command \includegraphics{FILENAME}
%
\documentclass[11pt]{article}
\usepackage{float}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{ragged2e}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}
\setlength{\headheight}{13.6pt}
\newcommand\question[2]{\vspace{.25in}\hrule\textbf{#1: #2}\vspace{.5em}\hrule\vspace{.10in}}
\renewcommand\part[1]{\vspace{.20in}\textbf{(#1)}}
\newcommand\algorithm{\vspace{.20in}\textbf{Algorithm: }}
\newcommand\correctness{\vspace{.20in}\textbf{Correctness: }}
\newcommand\runtime{\vspace{.20in}\textbf{Running time: }}
\pagestyle{fancyplain}
\lhead{\textbf{\NAME\ (\ANDREWID)}}
\chead{\textbf{HW\HWNUM}}
\rhead{11-661, \today}
\begin{document}\raggedright
%Section A==============Change the values below to match your information==================
\newcommand\NAME{Yuzhu Zhang}  % your name
\newcommand\ANDREWID{yuzhuz}     % your andrew id
\newcommand\HWNUM{6}              % the homework number
%Section B==============Put your answers to the questions below here=======================

% no need to restate the problem --- the graders know which problem is which,
% but replacing "The First Problem" with a short phrase will help you remember
% which problem this is when you read over your homeworks to study.

Did you receive any help whatsoever from anyone in solving this assignment? No.\\
Did you give any help whatsoever to anyone in solving this assignment? No.\\
Did you find or come across code that implements any part of this assignment ? No.\\

\question{1}{} 
\begin{align*}
I(X;Y) &= H(X)-H(X|Y)\\
 &= -\sum_{x}p(x)log(p(x)) + \sum_{x,y}p(x,y)log(p(x|y))\\
 &= -\sum_{x}p(x)log(p(x)) + \sum_{x,y}p(x,y)log(\frac{p(y|x)p(x)}{p(y)})\\
 &= -\sum_{x}p(x)log(p(x)) + \sum_{x,y}p(x,y)log(p(y|x)) + \sum_{x,y}p(x,y)log(p(x)) - \sum_{x,y}p(x,y)log(p(y))  \\
 &= \sum_{x,y}p(x,y)log(p(y|x)) - \sum_{x,y}p(x,y)log(p(y)) \\
	& = H(Y) - H(Y|X)\\
	& = I(Y;X)
\end{align*}

\question{3}{}
\part{a}
\begin{center}
	\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
  s & 2& 3& 4& 5& 6&  7& 8& 9&  10& 11& 12\\
  \hline
   & $\frac{1}{36}$& $\frac{2}{36}$& $\frac{3}{36}$& $\frac{4}{36}$& $\frac{5}{36}$&  $\frac{6}{36}$& $\frac{5}{36}$& $\frac{4}{36}$&  $\frac{3}{36}$& $\frac{2}{36}$& $\frac{1}{36}$\\
\end{tabular}
\end{center}
\begin{align*}
	H(S) &= \sum_{s}p(s)log(\frac{1}{p(s)})\\
		&= (\frac{1}{36}+\frac{1}{36})log(36) + (\frac{2}{36}+\frac{2}{36})log(\frac{36}{2}) + (\frac{3}{36}+\frac{3}{36})log(\frac{36}{3})+\\&(\frac{4}{36}+\frac{4}{36})log(\frac{36}{4}) + (\frac{5}{36}+\frac{5}{36})log(\frac{36}{5}) + \frac{6}{36}log(\frac{36}{6})\\
		&=  3.2744
\end{align*}

\part{b}
Let $W_{i}$ be the different choice (letter or digit) in position i,since all positions are independent, then the entropy of a sequence of choice is : \\
\begin{align*}
H(W_{1}...W_{n}) &= \sum_{i=1}^{n}H(W_{i}|W_{1}...W_{i-1})\\
&=\sum_{i=1}^{n}H(W_{i}) = nH(W_{i})
\end{align*}
For a single choice $W_{i}$, let A be the choice of letter or digit, p(A = letter) = $\lambda$, p(A = digit) = $1-\lambda$ , X be the choice within digit or letter then\\
\begin{align*}
	H(W_{i}) &= H(X,A)
	\\&= H(X|A) + H(A)
	\\&= p(A = digit)H(X|A = digit) +p(A = letter)H(X|A = letter) + H(A)
\end{align*}
\begin{align*}
H(A) = H(\lambda),H(X|A = digit) = H_{D},H(X|A = letter) = H_{L}\\
\end{align*}
So
\begin{align*}
H(W_{i}) &= (1-\lambda)H_{D} + \lambda H_{L} + H(\lambda)\\
H_{p} &= H(W_{1}...W_{n})\\ &= n * H(W_{i})\\ &= n*((1-\lambda)H_{D} + \lambda H_{L} + H(\lambda))
\end{align*}
\question{4}{}
X = {0,1},Y = {0,1} Z = {0,1}
\begin{center}
	\begin{tabular}{c|c|c|c|c|c|c|c}
  (0,0,0) & (0,0,1)& (0,1,0)& (0,1,1)& (1,0,0)& (1,0,1)&  (1,1,0)& (1,1,1)\\
  \hline
   0.25& 0 & 0 &0.25& 0& 0.25& 0.25& 0\\ 
\end{tabular}
\end{center}
Here p(X = 0) = 0.5, p(X = 1) = 0.5, p(Y = 0) = 0.5, p(Y = 1) = 0.5, p(Z= 0) = 0.5, p(Z = 1) = 0.5\\
p(X = 0,Z = 0) = p(X = 0)p(Z = 0) = 0.25\\
p(X = 0,Z = 1) = p(X = 0)p(Z = 1) = 0.25\\
p(X = 1,Z = 0) = p(X = 1)p(Z = 0) = 0.25\\
p(X = 1,Z = 1) = p(X = 1)p(Z = 1) = 0.25\\
X Z are independent, (a) is satisfied\\
p(Y = 0,Z = 0) = p(Y = 0)p(Z = 0) = 0.25\\
p(Y = 0,Z = 1) = p(Y = 0)p(Z = 1) = 0.25\\
p(Y = 1,Z = 0) = p(Y = 1)p(Z = 0) = 0.25\\
p(Y = 1,Z = 1) = p(Y = 1)p(Z = 1) = 0.25\\
Y Z are independent, (b) is satisfied\\
H(Z) = -0.5 *log(0.5) -0.5*log(0.5) = 1\\
I(X,Y;Z) = 4* 0.25 * log(0.25/(0.25*0.5)) = 1\\
(c) is satisfied
\question{5}{}
\part{a}
Entropy = 8.75292253708\\
\part{b}
The probability for word which is not in the sample, the Pml = 0 , which will run into problem when compute the cross entropy because CH requires take log to probability. To fix it, we can use the smoothing technique in the below question, add pseudo count or shrinkage for word not exist in the sample\\
\part{c}
pseudo count(add 1 for not sampled word):8.77048363953\\
shrinkage:8.78714684234\\
\part{d}

\justify
The smoothing parameter in pseudo count is the count for word which does not exist in the sample. The larger pseudo count it is, the smaller the weight of the word count from the sample. In that case, the distribution after pseudo count smooth is less likely to reflect the word distribution from sample. So the cross entropy increases because the smoothed distribution has larger distance from the true probability. Similarly, the larger N it is, the less impact of the pseudo count it has because the term frequency from sample is large compared with pseudo count. Larger N also make sample more likely to approximate true probability. That's why larger N has small cross entropy and the change in cross entropy is smaller.\\
The smoothing parameter in shrinkage count is the weight for word which exist in the sample. The larger weight it is, the distribution after smoothing is less likely to take effect because the weight of unseen word is smaller. So the cross entropy decrease because the smoothed effect is smaller and unseen word tends to have smaller probability. Similarly, larger N also make sample more likely to approximate true probability. That's why larger N has small cross entropy.\\
\question{6}{}
N is the sum of the total count\\
\begin{align*}
	H &= \sum_{word_i} p(word_i)*log(\frac{1}{p(word_i)}) \\&= \sum_{word_i} \frac{count(word_i)}{N} log(\frac{N}{count(word_i)})\\
	&= \frac{1}{N} \sum(count*log(N)-count*log(count))\\
	&= \frac{1}{N}log(N)\sum(count) - \frac{1}{N}\sum(count*log(count))\\
	&= log(N) - \frac{1}{N}\sum_{word_i}(count(word_i)*log(count(word_i)))
\end{align*}
\justify
So the input file can be treated as a data stream, we use a variable to keep track of the $\sum{word_i}(count(word_i)*log(count(word_i)))$ , then every time a new count is read, the variable is updated, also the number of total count is updated too, after we read all the data , we can use the above formula to compute the entropy.
\end{document}
